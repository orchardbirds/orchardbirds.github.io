{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"Welcome to the bokbokbok doks! \u00b6 bokbokbok is a Python library that lets us easily implement custom loss functions and eval metrics in LightGBM and XGBoost. Installation \u00b6 In order to install bokbokbok you need to use Python 3.6 or higher. Install bokbokbok via pip with: pip install bokbokbok Alternatively you can fork/clone and run: git clone https://gitlab.com/orchardbirds/bokbokbok.git cd bokbokbok pip install . Licence \u00b6 todo","title":"Home"},{"location":"index.html#welcome-to-the-bokbokbok-doks","text":"bokbokbok is a Python library that lets us easily implement custom loss functions and eval metrics in LightGBM and XGBoost.","title":"Welcome to the bokbokbok doks!"},{"location":"index.html#installation","text":"In order to install bokbokbok you need to use Python 3.6 or higher. Install bokbokbok via pip with: pip install bokbokbok Alternatively you can fork/clone and run: git clone https://gitlab.com/orchardbirds/bokbokbok.git cd bokbokbok pip install .","title":"Installation"},{"location":"index.html#licence","text":"todo","title":"Licence"},{"location":"derivations/focal.html","text":"Weighted Focal Loss \u00b6 Weighted Focal Loss applies a scaling parameter alpha and a focusing parameter gamma to Binary Cross Entropy We take the definition of the Focal Loss from this paper : where: This is equivalent to writing: We calculate the Gradient: We also need to calculate the Hessian: By setting alpha = 1 and gamma = 0 we obtain the Gradient and Hessian for Binary Cross Entropy Loss, as expected.","title":"Focal Loss"},{"location":"derivations/focal.html#weighted-focal-loss","text":"Weighted Focal Loss applies a scaling parameter alpha and a focusing parameter gamma to Binary Cross Entropy We take the definition of the Focal Loss from this paper : where: This is equivalent to writing: We calculate the Gradient: We also need to calculate the Hessian: By setting alpha = 1 and gamma = 0 we obtain the Gradient and Hessian for Binary Cross Entropy Loss, as expected.","title":"Weighted Focal Loss"},{"location":"derivations/log_cosh.html","text":"Log Cosh Error \u00b6 The equation for Log Cosh Error is: We calculate the Gradient: We also need to calculate the Hessian:","title":"Log Cosh Error"},{"location":"derivations/log_cosh.html#log-cosh-error","text":"The equation for Log Cosh Error is: We calculate the Gradient: We also need to calculate the Hessian:","title":"Log Cosh Error"},{"location":"derivations/note.html","text":"A Note About Gradients in Classification Problems \u00b6 For the gradient boosting packages we have to calculate the gradient of the Loss function with respect to the marginal probabilites . In this case, we must calculate The Hessian is similarly calculated: Where y-hat is the sigmoid function, unless stated otherwise : We will make use of the following property for the calculations of the Losses and Hessians:","title":"General Remarks"},{"location":"derivations/note.html#a-note-about-gradients-in-classification-problems","text":"For the gradient boosting packages we have to calculate the gradient of the Loss function with respect to the marginal probabilites . In this case, we must calculate The Hessian is similarly calculated: Where y-hat is the sigmoid function, unless stated otherwise : We will make use of the following property for the calculations of the Losses and Hessians:","title":"A Note About Gradients in Classification Problems"},{"location":"derivations/sle.html","text":"Squared Log Error \u00b6 The equation for Squared Log Error is: We calculate the Gradient: We also need to calculate the Hessian:","title":"Squared Log Error"},{"location":"derivations/sle.html#squared-log-error","text":"The equation for Squared Log Error is: We calculate the Gradient: We also need to calculate the Hessian:","title":"Squared Log Error"},{"location":"derivations/wce.html","text":"Weighted Cross Entropy Loss \u00b6 Weighted Cross Entropy applies a scaling parameter alpha to Binary Cross Entropy , allowing us to penalise false positives or false negatives more harshly. If you want false positives to be penalised more than false negatives, alpha must be greater than 1. Otherwise, it must be less than 1. The equations for Binary and Weighted Cross Entropy Loss are the following: We calculate the Gradient: We also need to calculate the Hessian: By setting alpha = 1 we obtain the Gradient and Hessian for Binary Cross Entropy Loss, as expected.","title":"Weighted Cross Entropy"},{"location":"derivations/wce.html#weighted-cross-entropy-loss","text":"Weighted Cross Entropy applies a scaling parameter alpha to Binary Cross Entropy , allowing us to penalise false positives or false negatives more harshly. If you want false positives to be penalised more than false negatives, alpha must be greater than 1. Otherwise, it must be less than 1. The equations for Binary and Weighted Cross Entropy Loss are the following: We calculate the Gradient: We also need to calculate the Hessian: By setting alpha = 1 we obtain the Gradient and Hessian for Binary Cross Entropy Loss, as expected.","title":"Weighted Cross Entropy Loss"},{"location":"tutorials/focal_loss.html","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); from sklearn.datasets import make_classification from sklearn.model_selection import train_test_split from bokbokbok.loss_functions.classification import FocalLoss from bokbokbok.eval_metrics.classification import FocalMetric X , y = make_classification ( n_samples = 1000 , n_features = 10 , random_state = 41114 ) X_train , X_valid , y_train , y_valid = train_test_split ( X , y , test_size = 0.25 , random_state = 41114 ) alpha = 0.7 gamma = 2 Usage in LightGBM \u00b6 import lightgbm as lgb train = lgb . Dataset ( X_train , y_train ) valid = lgb . Dataset ( X_valid , y_valid , reference = train ) params = { 'n_estimators' : 300 , 'seed' : 41114 , 'n_jobs' : 8 , 'learning_rate' : 0.1 , } clf = lgb . train ( params = params , train_set = train , valid_sets = [ train , valid ], valid_names = [ 'train' , 'valid' ], fobj = FocalLoss ( alpha = alpha , gamma = gamma ), feval = FocalMetric ( alpha = alpha , gamma = gamma ), early_stopping_rounds = 100 ) Usage in XGBoost \u00b6 import xgboost as xgb dtrain = xgb . DMatrix ( X_train , y_train ) dvalid = xgb . DMatrix ( X_valid , y_valid ) params = { 'seed' : 41114 , 'learning_rate' : 0.1 , 'disable_default_eval_metric' : 1 } bst = xgb . train ( params , dtrain = dtrain , num_boost_round = 300 , early_stopping_rounds = 10 , verbose_eval = 10 , obj = FocalLoss ( alpha = alpha , gamma = gamma ), maximize = False , feval = FocalMetric ( alpha = alpha , gamma = gamma , XGBoost = True ), evals = [( dtrain , 'dtrain' ), ( dvalid , 'dvalid' )])","title":"Focal Loss"},{"location":"tutorials/focal_loss.html#usage-in-lightgbm","text":"import lightgbm as lgb train = lgb . Dataset ( X_train , y_train ) valid = lgb . Dataset ( X_valid , y_valid , reference = train ) params = { 'n_estimators' : 300 , 'seed' : 41114 , 'n_jobs' : 8 , 'learning_rate' : 0.1 , } clf = lgb . train ( params = params , train_set = train , valid_sets = [ train , valid ], valid_names = [ 'train' , 'valid' ], fobj = FocalLoss ( alpha = alpha , gamma = gamma ), feval = FocalMetric ( alpha = alpha , gamma = gamma ), early_stopping_rounds = 100 )","title":"Usage in LightGBM"},{"location":"tutorials/focal_loss.html#usage-in-xgboost","text":"import xgboost as xgb dtrain = xgb . DMatrix ( X_train , y_train ) dvalid = xgb . DMatrix ( X_valid , y_valid ) params = { 'seed' : 41114 , 'learning_rate' : 0.1 , 'disable_default_eval_metric' : 1 } bst = xgb . train ( params , dtrain = dtrain , num_boost_round = 300 , early_stopping_rounds = 10 , verbose_eval = 10 , obj = FocalLoss ( alpha = alpha , gamma = gamma ), maximize = False , feval = FocalMetric ( alpha = alpha , gamma = gamma , XGBoost = True ), evals = [( dtrain , 'dtrain' ), ( dvalid , 'dvalid' )])","title":"Usage in XGBoost"},{"location":"tutorials/weighted_cross_entropy.html","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); from sklearn.datasets import make_classification from sklearn.model_selection import train_test_split from bokbokbok.loss_functions.classification import WeightedCrossEntropyLoss from bokbokbok.eval_metrics.classification import WeightedCrossEntropyMetric X , y = make_classification ( n_samples = 1000 , n_features = 10 , random_state = 41114 ) X_train , X_valid , y_train , y_valid = train_test_split ( X , y , test_size = 0.25 , random_state = 41114 ) alpha = 0.7 Usage in LightGBM \u00b6 import lightgbm as lgb train = lgb . Dataset ( X_train , y_train ) valid = lgb . Dataset ( X_valid , y_valid , reference = train ) params = { 'n_estimators' : 300 , 'seed' : 41114 , 'n_jobs' : 8 , 'learning_rate' : 0.1 , } clf = lgb . train ( params = params , train_set = train , valid_sets = [ train , valid ], valid_names = [ 'train' , 'valid' ], fobj = WeightedCrossEntropyLoss ( alpha = alpha ), feval = WeightedCrossEntropyMetric ( alpha = alpha ), early_stopping_rounds = 100 ) Usage in XGBoost \u00b6 import xgboost as xgb dtrain = xgb . DMatrix ( X_train , y_train ) dvalid = xgb . DMatrix ( X_valid , y_valid ) params = { 'seed' : 41114 , 'learning_rate' : 0.1 , 'disable_default_eval_metric' : 1 } bst = xgb . train ( params , dtrain = dtrain , num_boost_round = 300 , early_stopping_rounds = 10 , verbose_eval = 10 , obj = WeightedCrossEntropyLoss ( alpha = alpha ), maximize = False , feval = WeightedCrossEntropyMetric ( alpha = alpha , XGBoost = True ), evals = [( dtrain , 'dtrain' ), ( dvalid , 'dvalid' )])","title":"Weighted Cross Entropy"},{"location":"tutorials/weighted_cross_entropy.html#usage-in-lightgbm","text":"import lightgbm as lgb train = lgb . Dataset ( X_train , y_train ) valid = lgb . Dataset ( X_valid , y_valid , reference = train ) params = { 'n_estimators' : 300 , 'seed' : 41114 , 'n_jobs' : 8 , 'learning_rate' : 0.1 , } clf = lgb . train ( params = params , train_set = train , valid_sets = [ train , valid ], valid_names = [ 'train' , 'valid' ], fobj = WeightedCrossEntropyLoss ( alpha = alpha ), feval = WeightedCrossEntropyMetric ( alpha = alpha ), early_stopping_rounds = 100 )","title":"Usage in LightGBM"},{"location":"tutorials/weighted_cross_entropy.html#usage-in-xgboost","text":"import xgboost as xgb dtrain = xgb . DMatrix ( X_train , y_train ) dvalid = xgb . DMatrix ( X_valid , y_valid ) params = { 'seed' : 41114 , 'learning_rate' : 0.1 , 'disable_default_eval_metric' : 1 } bst = xgb . train ( params , dtrain = dtrain , num_boost_round = 300 , early_stopping_rounds = 10 , verbose_eval = 10 , obj = WeightedCrossEntropyLoss ( alpha = alpha ), maximize = False , feval = WeightedCrossEntropyMetric ( alpha = alpha , XGBoost = True ), evals = [( dtrain , 'dtrain' ), ( dvalid , 'dvalid' )])","title":"Usage in XGBoost"}]}